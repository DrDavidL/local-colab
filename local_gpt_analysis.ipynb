{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local GPT Analysis for Medical Students\n",
    "\n",
    "## ü§ñ Analyzing Data with Natural Language Questions\n",
    "\n",
    "This notebook demonstrates how to use **local large language models** to analyze medical data using plain English questions. Unlike cloud-based AI services, everything runs on your computer, ensuring complete data privacy.\n",
    "\n",
    "### Why This Approach Matters for Medical Students:\n",
    "- üîí **Complete Privacy**: Your data never leaves your computer\n",
    "- üí¨ **Natural Language**: Ask questions in plain English, no complex coding required\n",
    "- üè• **Medical Context**: Perfect for exploring healthcare datasets securely\n",
    "- üìä **Instant Analysis**: Get immediate insights and visualizations\n",
    "\n",
    "### Important Notes:\n",
    "- **Always consult your PI** about methods, devices, and approach before analyzing real data\n",
    "- This method complements other data science techniques covered in separate notebooks\n",
    "- The local model (qwen3) works on older devices but must be downloaded first via Ollama\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Prerequisites\n",
    "\n",
    "### Required Setup:\n",
    "\n",
    "1. **Install Ollama** from https://ollama.ai\n",
    "2. **Download the qwen3:8b model**:\n",
    "   ```bash\n",
    "   ollama pull qwen3:8b\n",
    "   ```\n",
    "3. **Install requirements** per the README.md file\n",
    "\n",
    "### ‚è±Ô∏è First-Time Usage Note:\n",
    "**The first time you use the model, it takes longer to load** - don't be discouraged! Subsequent queries will be much faster as the model stays loaded in memory.\n",
    "\n",
    "### Why qwen3:8b?\n",
    "- Runs efficiently on older hardware\n",
    "- Good balance of performance and resource usage\n",
    "- Excellent for data analysis tasks\n",
    "- Completely offline operation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Setting Up Your Local Analysis Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all requirements first per the README.md file\n",
    "# You can use the same virtual environment as the one used for the local Exploring Data notebook.\n",
    "\n",
    "from pandasai_litellm.litellm import LiteLLM\n",
    "import pandasai as pai\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pai.read_csv(\"data/predictdm.csv\")\n",
    "\n",
    "print(f\"üìä Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(\"Ready for natural language analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the local language model\n",
    "# Note: First run may take 30-60 seconds to load the model\n",
    "model = LiteLLM(model=\"ollama/qwen3:8b\")\n",
    "\n",
    "pai.config.set({\n",
    "    \"llm\": model,\n",
    "    \"save_charts\": False\n",
    "})\n",
    "\n",
    "print(\"ü§ñ Local language model ready!\")\n",
    "print(\"üí° Tip: First query may be slow while model loads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Natural Language Data Analysis\n",
    "\n",
    "Now you can ask questions about your data in plain English! The local AI will interpret your questions and generate appropriate analysis code.\n",
    "\n",
    "### Example Questions You Can Ask:\n",
    "- \"What is the average age of patients?\"\n",
    "- \"Show me the distribution of glucose levels\"\n",
    "- \"Compare BMI between diabetic and non-diabetic patients\"\n",
    "- \"Create a scatter plot of age vs glucose\"\n",
    "- \"What percentage of patients have diabetes?\"\n",
    "\n",
    "### Let's Start with Basic Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about mean glucose for men\n",
    "# This demonstrates how natural language gets converted to data analysis\n",
    "# The first question takes longer as the model loads\n",
    "response = df.chat('What is the mean glucose for men?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations with natural language\n",
    "chart_response = df.chat(\"Plot age distribution\")\n",
    "print(chart_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex analysis with regression\n",
    "plot_response = df.chat(\"Plot glucose vs age with a regression line\")\n",
    "print(plot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Try Your Own Questions\n",
    "\n",
    "Use the cells below to ask your own questions about the diabetes dataset. Remember, you can ask in natural language!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your question here - try asking about different variables or relationships\n",
    "# Example: df.chat(\"What is the correlation between BMI and diabetes?\")\n",
    "\n",
    "your_response = df.chat(\"Your question here\")\n",
    "print(your_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another question - try asking for a different type of visualization\n",
    "# Example: df.chat(\"Create a box plot of glucose levels by gender\")\n",
    "\n",
    "another_response = df.chat(\"Your visualization request here\")\n",
    "print(another_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Advanced Natural Language Queries\n",
    "\n",
    "The local LLM can handle more sophisticated analysis requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex statistical analysis\n",
    "complex_analysis = df.chat(\"Calculate the diabetes prevalence by age groups: under 30, 30-50, 50-70, and over 70\")\n",
    "print(complex_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-variable analysis\n",
    "multi_var = df.chat(\"Show me a correlation heatmap of all numeric variables\")\n",
    "print(multi_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîí Security and Privacy Benefits\n",
    "\n",
    "### Why Local LLMs Matter for Medical Data:\n",
    "\n",
    "‚úÖ **Complete Data Privacy**: No data transmission to external servers  \n",
    "‚úÖ **HIPAA Compliance**: Maintains patient confidentiality  \n",
    "‚úÖ **Institutional Control**: Meets hospital/university data policies  \n",
    "‚úÖ **Audit Trail**: All analysis happens on your controlled environment  \n",
    "‚úÖ **No Internet Required**: Works offline once model is downloaded  \n",
    "\n",
    "### Best Practices:\n",
    "- Always discuss your analysis approach with your Principal Investigator\n",
    "- Ensure your device meets your institution's security requirements\n",
    "- Keep your local environment updated and secure\n",
    "- Document your analysis methods for reproducibility\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Tips for Effective Natural Language Queries\n",
    "\n",
    "### Good Question Patterns:\n",
    "- **Be specific**: \"Show glucose distribution for diabetic patients\" vs \"Show glucose\"\n",
    "- **Request context**: \"Compare average BMI between groups\" vs \"Calculate BMI\"\n",
    "- **Ask for visualizations**: \"Plot X vs Y\" or \"Create a histogram of Z\"\n",
    "- **Specify groupings**: \"Analyze by gender\", \"Group by age ranges\"\n",
    "\n",
    "### What the Local LLM Can Do:\n",
    "- Generate statistical summaries\n",
    "- Create various plot types (scatter, histogram, box plot, etc.)\n",
    "- Perform correlation analysis\n",
    "- Group and aggregate data\n",
    "- Calculate percentages and ratios\n",
    "- Filter and subset data\n",
    "\n",
    "### Performance Notes:\n",
    "- **First query**: May take 30-60 seconds (model loading)\n",
    "- **Subsequent queries**: Much faster (2-10 seconds)\n",
    "- **Complex requests**: May take longer but provide detailed analysis\n",
    "- **Model stays loaded**: Until you restart the notebook\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Next Steps\n",
    "\n",
    "This notebook focused on **natural language data analysis** using local LLMs. To expand your data science skills:\n",
    "\n",
    "1. **Explore other notebooks** in this collection for traditional data science methods\n",
    "2. **Practice with different datasets** to see how the LLM adapts\n",
    "3. **Combine approaches**: Use this for exploration, then traditional methods for detailed analysis\n",
    "4. **Learn prompt engineering**: Better questions lead to better analysis\n",
    "\n",
    "### Remember:\n",
    "- This is one tool in your data science toolkit\n",
    "- Always validate important findings with traditional statistical methods\n",
    "- Consult with your PI about appropriate use in research contexts\n",
    "- The local approach ensures your data remains secure and private\n",
    "\n",
    "**Happy analyzing with natural language! ü§ñüìäüîí**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
